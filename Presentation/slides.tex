\documentclass{beamer}
\usepackage{amsmath}
\usepackage{color}
\usepackage{ulb}
\title{Implementation of Decision Tree Classifiers\\
ID3 versus C4.5}
\author{Depuydt Antoine\\ Dany Efila \\Mudura Mircea}

\date{May, 2017}
\begin{document}
\maketitle

\addtobeamertemplate{navigation symbols}{}{%
    \usebeamerfont{footline}%
    \usebeamercolor[fg]{footline}%
    \hspace{1em}%
    \insertframenumber/\inserttotalframenumber
}

\section{Introduction}
\begin{frame}
\frametitle{Introduction}
\begin{itemize}
\item Data mining: compress, understand and predict
	\vfill
	\begin{itemize}
		\item Clustering
		\vfill
		\item Classification
		\vfill
		\item Regression
		\vfill
		\item ...
	\end{itemize}
\vfill
\item Techniques to find links
	\vfill
	\begin{itemize}
		\item Linear Regression
		\vfill
		\item Decision Trees
		\vfill
		\item Neural Networks
		\vfill
		\item ...
	\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Classification}
	\begin{itemize}
		\item Classical example: play tennis today?
		\vfill
		\begin{itemize}
			\item \textbf{Features}:
			\begin{itemize}
				\item Outlook: sunny, overcast, rainy
				\vfill
				\item Temperature: hot, cool, cold
				\vfill
				\item Wind: high, weak
				\vfill
				\item Humidity: high, normal
			\end{itemize}
			\vfill
			\item \textbf{Class labels}:
				\begin{itemize}
				\item Yes
				\vfill
				\item No
				\end{itemize}			

		\end{itemize}
	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Decision Tree}
	\begin{itemize}
		\item Visual model, easily understandable
		\vfill
		\item Model: tree with \color{blue}decision \color{black}and \color{green}leaf nodes \color{black}
	\end{itemize}
	\begin{center}
		\includegraphics[scale=0.35]{Images/DecisionTree.png}
	\end{center}

\end{frame}

\begin{frame}
\frametitle{Premise}
\begin{itemize}
\item Given a training data-set 
\vfill
\item Recursively split on a node:
\vfill
\item If node is pure return leaf (class value)
\vfill
\item Else compute entropy \& info gain:
\begin{itemize}
\vfill
\item Shannon's entropy: $E(S)= \sum_{i}{} - p_i log_2(p_i) $
\vfill
\item Subtree gain: $Gain(T,X)=E(T)-E(T,X)$
\vfill
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{ID3 versus C4.5}
	\begin{itemize}
		\item Goal: implement ID3 and C4.5 algorithms
		\vfill
		\item Objectives: compare ID3 and C4.5 output
		\vfill
		\begin{itemize}
			\item Compare ID3 and C4.5
			\vfill
			\item Create an application that classifies any data using both algorithms
		\end{itemize}

	\end{itemize}

\end{frame}

\input{id3}
\begin{frame}
\frametitle{ID3 - Learning algorithm}
\begin{itemize}
		\item  Make a decision based on measurement of probability
		\begin{itemize}
			\item How to decide? Split pocess
		\end{itemize}
		\item Iteration over values of an attribute:
		\begin{itemize}
			\item Choose a value
			\item Assign a classification
		\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{ID3 - Split procedure}
\begin{itemize}
		\item  How to decide which node to split on? 
		\item Two main elements:
		\begin{itemize}
			\item Entropy
			\item Information gain
		\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{ID3 - Entropy}
\begin{itemize}
		\item  Given S a sample of training examples
		\item Two classes of examples:
		\begin{itemize}
			\item Proportion of positive examples $\textrm{P}_\textrm{+}$
			\item Proportion of negatives examples $\textrm{P}_\textrm{-}$
		\end{itemize}
		\item Measure of impurety of S
		\begin{itemize}
			\item number of bits need to encode each class
			\item Formula: $E(S)= \sum_{i}{} - p_i log_2(p_i) $
		\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{ID3 - Information gain}
\begin{itemize}
		\item  Difference in the entropy before split and after split 
		\item Sum of weighted entropy:  $Gain(T,X)=E(T)-E(T,X)$
		\begin{itemize}
			\item $Gain(T,X)> 0$ -> gain of certainty
			
		\end{itemize}

	\end{itemize}
\end{frame}

\begin{frame}
\frametitle{ID3 - Pseudo code example}
\begin{itemize}
	\item If there is a mix of positive and negative examples ID3 works as:
	\item ID3 (Examples, Target_Attribute, Attributes)
      \item   A ← The Attribute that best classifies examples.
       \item  Decision Tree attribute for Root = A.
       \item  For each possible value, vi, of A,
          \item   Add a new tree branch below Root, corresponding to the test A = vi.
            \item Let Examples(vi) be the subset of examples that have the value vi for A
            \item If Examples(vi) is empty
              \item   Then below this new branch add a leaf node with label = most common target value in the examples
           \item  Else below this new branch add the subtree ID3 (Examples(vi), Target_Attribute, Attributes – {A})
   \item  End
   \item  Return Root

	\end{itemize}
\end{frame}

\input{c45}

\begin{frame}
\frametitle{K-fold cross validation}

\end{frame}

\begin{frame}
\frametitle{Demonstration}

\end{frame}

\end{document}
